\BB\ (Single- or Multi-Process Optimization over Networks) Version 3.0
is a powerful environment for implementing branch, cut, and price
algorithms. The subroutines in the \BB\ library comprise a
state-of-the-art solver which is designed to be completely modular and
easy to port to various problem settings. All library subroutines are
generic---their implementation does not depend on the the
problem-setting. To develop a full-scale, parallel branch and cut
algorithm, the user has only to specify a few problem-specific
functions such as preprocessing and separation. The vast majority of
the computation takes place within a ``black box,'' of which the user
need have no knowledge. \BB\ communicates with the user's routines
through well-defined interfaces and performs all the normal functions
of branch and cut---tree management, LP solution, cut pool management,
as well as inter-process or inter-thread communication. Although there
are default options, the user can also assert control over the
behavior of \BB\ through a myriad of parameters and optional
subroutines. \BB\ can be built in a variety of configurations, ranging
from fully parallel to completely sequential, depending on the user's
needs. The library runs serially on almost any platform, and can also
run in parallel in either a fully distributed environment (network of
workstations) or shared-memory environment simply by changing a few
options in the make file. To run in a distributed environment, the
user must have installed {\em \htmladdnormallink{Parallel Virtual
Machine}{http://www.ccs.ornl.gov/pvm/}} (PVM) software, available for
free from Oak Ridge National Laboratories
\begin{latexonly}
at {\tt http://www.ccs.ornl.gov/pvm/} 
\end{latexonly}. 
To run in a shared memory environment, the user must have installed an
OpenMP compliant compiler. A cross-platform compiler called {\em
\htmladdnormallink{Omni}{http://pdplab.trc.rwcp.or.jp/Omni}}, which uses 
{\tt cc} or {\tt gcc} as a back end, is available for free download
\begin{latexonly}
at {\tt http://pdplab.trc.rwcp.or.jp/Omni}.
\end{latexonly}.
This section of the manual is concerned with the detailed
specifications needed to develop an application using \BB. It is
assumed that the user has already read the first part of the manual, which
provides a high-level introduction to parallel branch, cut, and price
and the overall design and use of \BB. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{New in Version 3.0}

Here are the additions for Version 3.0:

\begin{itemize}

\item An interface to the LP solver OSL is now included. Be forewarned
that this interface is a beta version. Although it seems to perform
properly, it currently causes the branch and bound tree to grow
significantly over what would be experienced with CPLEX. This anomaly
has yet to be explained. Use with caution. 

\item A few small bugs were fixed.

\item Added support for ccmalloc to the makefile.

\item Added support for starting up the drawgraph process on a specific
machine. 

\item Added support for new distance types in the VRP application.

\item Added {\tt ROOT\_NODE\_ONLY} compiler directive which directs
the solver to process only the root node of the branch and bound tree.
This is useful for research purposes when one wants to test the change
in lower bound in the root node after the addition of some cutting
plane procedures.

\item Added ``deletable'' field to the data structure for cuts. This field
allows the user to specify that certain cuts should never be removed
after being added. It is set to TRUE by default which maintains the
legacy behavior.

\item Added support for compiling with MS Visual Studio 6.0.

\end{itemize}

\subsection{New in Version 2.8.1}

Here are the additions for Version 2.8.1:

\begin{itemize}

\item The LP solver interface has been upgraded to accomodate CPLEX
Version 7.0. There have also been minor fixes to the interface.

\item A few small bugs were fixed.

\end{itemize}

\subsection{New in Version 2.8}

If you are new to \BB, you can skip to Section
\ref{getting_started}. Here is a list the new features available in
SYMPHONY 2.8:
\begin{itemize}

        \item {\bf New search rules}. There are new search rules
        available in the tree manager. These rules enable better control
        of \hyperref{diving}{diving (see Section }{)}{diving}.

        \item {\bf More accurate timing information}. Reported timing
        information is now more accurate.

        \item {\bf Idle Time Reporting}. Measures of processor idle
        time are now reported in the run statistics.

        \item {\bf More efficient cut pool management}. Cuts are now
        optionally ranked and purged according to a user-defined
        measure of quality. See the description of \hyperref{\tt
        user\_check\_cut()} {{\tt user\_check\_cut()} (Section } {)}
        {user_check_cuts}.

        \item {\bf Easier use of built-in branching functions}.
        Built-in branching functions can now be more easily called
        directly by the user if desired. Previously, these functions
        required the passing of internal data structures, making them
        difficult for the user to call directly. See the functions
        branch\_* in the file {\tt LP/lp\_branch.c} for usage.

        \item {\bf Better control of strong branching}. A new strong
        branching strategy allows the user to specify that more strong
        branching candidates should be used near the top of the tree
        where branching decisions are more critical. See the
        description of the relevant \hyperref{parameters} {parameters (Section
        }{)}{strong_branching}.

\end{itemize}

\subsection{Changes to the User API in Version 2.8}

There are some minor changes to the user interface in order to allow
the use of the new features. If you have code written for an older
version, you will have to make some very minor modifications before
compiling with version 2.8.

\begin{itemize}
        
        \item \hyperref{\tt \bf user\_start\_heurs()} {{\tt
        user\_start\_heurs()} (Section} {)}{user_start_heurs} now
        includes as an optional return value a user-calculated
        estimate of the optimal upper bound. This estimate is used to
        control diving. See the description of the new
        \hyperref{diving rules}{diving rules (see Section }{)}{diving}
        for more information. Since this return value is optional, you
        need only add the extra argument to your function definition to
        upgrade to the 2.8 interface. No changes to your code are
        required. 
        
        \item \hyperref{\tt \bf user\_check\_cut()} {{\tt
        user\_check\_cut()} (Section} {)}{user_check_cuts} now
        includes as an optional return value a user-defined assessment
        of the current quality of the cut. Since this return value is
        optional, you need only add the extra argument to your
        function definition to upgrade to the 2.8 interface. No
        changes to your code are required.
        
        \item \hyperref{\tt \bf user\_select\_candidates()} {{\tt
        user\_select\_candidates()} (Section}
        {)}{user_select_candidates} now passes in the value of the
        current level in the tree in case the user wants to use this
        information to make branching decisions. Again, the new
        argument just needs to be added to the function definition. No
        changes to your code are required.

\end{itemize}

\subsection{Getting Started}
\label{getting_started}

\subsubsection{Compiling the Sample Application with Unix Operating Systems}

Here is a sketch outline of how to get started with \BB\ in Unix. This is
basically the same information contained in the README file that comes with
the distribution and will lead you through the steps required to compile the
sample application, a VRP and TSP solver. Because \BB\ is intended to run over
nonhomogeneous networks of workstations, installation is not fully automated,
but requires the user to make minor edits to the makefile.  With this setup,
compilation for multiple architectures and configurations can be done in a
single directory without reconfiguring or ``cleaning.'' This is convenient on
nonhomogeneous networks, but it means that you might need to know a little
about your computing environment in order to make \BB\ compile. For the casual
user, this is limited to editing providing some path names. There may be some
complaints from the compiler because of missing function prototypes, etc.

\paragraph{Preparing for Sample Compilation.}

\begin{itemize}

        \item Download the file {\tt SYMPHONY-3.0.tgz}.

        \item Unpack the distribution with ``{\tt tar -xzf
        SYMPHONY-3.0.tgz}''. This will create a subdirectory called
        {\tt SYMPHONY-3.0/} containing the distribution.
        
        \item Edit the makefile ({\tt SYMPHONY-3.0/Makefile}) to reflect your
	environment. Only minor edits should be required. An explanation of
	what has to be set is contained in the comments in the makefile.

\end{itemize}
        
\paragraph{Compiling the Sequential Version.}

\begin{itemize}
        \item Type ``{\tt make}'' in the {\tt SYMPHONY-3.0/} directory. This will
first make the \BB\ library (sequential version). After making the \BB\
library, make will compile the user code and link the executable for the
sample application, a vehicle routing and traveling salesman problem
solver. The name of the executable will be 
\begin{center}
{\tt SYMPHONY-3.0/Vrp/bin.\$(ARCH)/master\_tm\_lp\_cg\_cp}, 
\end{center}
indicating that all
modules are contained in a single executable. The makefile must be modified to
enable parallel execution of the code.

\item After the code is compiled, you are free to type ``{\tt make
        clean}'' if you want to save disk space. You
        should only have to remake the library if you change something
        in \BB's internal files.

\item To test the sample program, a sample data file is included with the
distribution. Type 
\begin{center}
{\tt Vrp/bin.\$(ARCH)/master\_tm\_lp\_cg\_cp -F sample.vrp
-N 5 -u 522} 
\end{center}
in the {\tt SYMPHONY-3.0/} directory. The -N argument gives the number
of routes, which must be specified in advance. The -u argument supplies an
initial upper bound which is used until a better feasible solution is
found. TSP instances can also be solved by adding the option -T TSP. In the
case of the TSP, the number of routes does not need to be specified. You can
get additional problem data files from from \htmladdnormallink{\tt
http://branchandcut.org/VRP/data/}{http://branchandcut.org/VRP/data/} or the
\\ 
\htmladdnormallink{TSPLIB}
{http://www.iwr.uni-heidelberg.de/iwr/comopt/software/TSPLIB95/}
\begin{latexonly}
        (http://www.iwr.uni-heidelberg.de/iwr/comopt/software/TSPLIB95/)
\end{latexonly}. \\
The file
format is described on the TSPLIB Web site.
\end{itemize}

\paragraph{Compiling the Shared Memory Version.}

\begin{itemize}
        \item To compile a shared memory version, obtain an OpenMP
        compliant compiler, such as \htmladdnormallink{Omni}
        {http://phase.etl.go.jp/Omni/}
\begin{latexonly} 
        (free from {\tt http://phase.etl.go.jp/Omni})
\end{latexonly}. 
        Other options are listed at \htmladdnormallink{the OpenMP Web
        site}{http://www.openmp.org}
\begin{latexonly}
        ({\tt http://www.openmp.org})
\end{latexonly}.

        \item Follow the instructions above for configuring the makefile. Set
the variable {\tt CC} to the compiler name in the make file and compile as
above. Note that if you have previously compiled the sequential version, then
you should first type {\tt make clean\_all}, as this version uses the same
directories.  With one thread allowed, it should run exactly the same as the
sequential version so there is no need to compile both versions.

        \item Voila, you have a shared memory parallel solver. As above, to
test the sample program, a sample data file is included with the
distribution. Type {\tt Vrp/bin.\$(ARCH)/master\_tm\_lp\_cg\_cp -F sample.vrp
-N 5 -u 522} in the {\tt SYMPHONY-3.0/} directory. The -N argument gives the number
of routes, which must be specified in advance. The -u argument supplies an
initial upper bound which is used until a better feasible solution is
found. TSP instances can also be solved by adding the option -T TSP. In the
case of the TSP, the number of routes does not need to be specified. You can
get additional problem data files from from \htmladdnormallink{\tt
http://branchandcut.org/VRP/data/} {http://branchandcut.org/VRP/data/} or the
\\ 
\htmladdnormallink{TSPLIB}
{http://www.iwr.uni-heidelberg.de/iwr/comopt/software/TSPLIB95/}
\begin{latexonly}
        (http://www.iwr.uni-heidelberg.de/iwr/comopt/software/TSPLIB95/)
\end{latexonly}. \\
The file format is described on the TSPLIB Web site.

\end{itemize}

\paragraph{Compiling the Distributed Version.}

\begin{itemize}
        \item If you wish to compile a distributed version of the code, obtain
and install the {\em \htmladdnormallink{Parallel Virtual
Machine}{http://www.csm.ornl.gov/pvm/}} (PVM) software, available for free
from Oak Ridge National Laboratories
\begin{latexonly}
        at {\tt http://www.ccs.ornl.gov/pvm/}
\end{latexonly}. 
        See Section \ref{PVM} for more notes on using PVM.
        
        \item In the makefile, be sure to set the {\tt COMM\_PROTOCOL}
        variable to {\tt PVM}. Also, change one or more of {\tt
        COMPILE\_IN\_TM}, {\tt COMPILE\_IN\_LP}, {\tt COMPILE\_IN\_CG}, and
        {\tt COMPILE\_IN\_CP}, to {\tt FALSE}, or you will end up with the
        sequential version. Various combinations of these variables will give
        you different configurations and different executables. See Section
        \ref{configuration} for more info on setting them. Also, be sure to
        set the path variables in the make file appropriately so that make can
        find the PVM library.

        \item Type ``{\tt make}'' in the SYMPHPONY-3.0 directory to
        make the distributed libraries. As in Step 1 of the sequential
        version, you may type ``{\tt make clean}'' after making the
        library. It should not have to remade again unless you modify
        \BB's internal files.

        \item After the \BB\ libraries, user code will be compiled and
        required executables linked.

        \item Make sure there are links from your
        \$PVM\_ROOT/bin/\$PVM\_ARCH/ directory to each of the
        executables in the Vrp/bin.\$(ARCH) directory. This is required
        by PVM.

        \item Start the PVM daemon by typing ``{\tt pvm}'' on the command line
        and then typing ``{\tt quit}''.

        \item To test the sample program, a sample data file is included with
the distribution. Type {\tt Vrp/bin.\$(ARCH)/master -F sample.vrp -N 5 -u
522} in the {\tt SYMPHONY-3.0} directory (note that the actual executable name may
not be ``{\tt master}'' if {\tt COMPILE\_IN\_TM} is set to {\tt TRUE}; see
Section \ref{configuration} for more information on executable names.). The -N
argument gives the number of routes, which must be specified in advance. The
-u argument supplies an initial upper bound which is used until a better
feasible solution is found. TSP instances can also be solved by adding the
option -T TSP. In the case of the TSP, the number of routes does not need to
be specified. You can get additional problem data files from from
\htmladdnormallink{\tt http://branchandcut.org/VRP/data/}
{http://branchandcut.org/VRP/data/} or the \\
\htmladdnormallink{TSPLIB}
{http://www.iwr.uni-heidelberg.de/iwr/comopt/software/TSPLIB95/}
\begin{latexonly}
        (http://www.iwr.uni-heidelberg.de/iwr/comopt/software/TSPLIB95/)
\end{latexonly}. \\
The file
format is described on the TSPLIB Web site.

\end{itemize}

\noindent This should result in the successful compilation of the sample
application. Once you have accomplished this much, you are well on
your way to having an application of your own. Don't be daunted by the
seemingly endless list of user function that you are about to
encounter. Most of them are optional or have default options. If you
get lost, consult the source code for the sample application to see
how it's done.

\subsubsection{Compiling the Sample Application with Microsoft Windows}

Here is a sketch outline of how to get started with \BB\ in Windows. This is
basically the same information contained in the README file that comes with
the distribution and will lead you through the steps required to compile the
sample application, a VRP and TSP solver. Direct support is provided for
compilation under MS Visual Studio 6.0. Compilation for other compilers should
also be possible. Note that the windows version has some limitations. Detailed
timing information is not currently provided.  Support is only provided for
running in sequential mode at this time.

\begin{itemize}

\item Download {\tt SYMPHONY-3.0.zip} and unzip the archive. This will
create a subdirectory called {\tt SYMPHONY-3.0$\backslash$} containing all 
the source files.

\item In MS Visual C++ 6.0, open the workspace 
{\tt SYMPHONY-3.0$\backslash$WIN32$\backslash$symphony.dsw}.  Note that
there are two projects, one called ``{\tt symphony}'' and one called ``{\tt
vrp}''.  The symphony project contains the source code needed to build the
internal library. The vrp project contains the source code for the
user-defined functions needed to build the sample application, a VRP and TSP
solver. Note that to develop a solver of your own, you would replace the VRP
library with one of your own.

\item By default, SYMPHONY is set up to use CPLEX 7.0 installed in a folder
called ``{\tt C:$\backslash$ILOG$\backslash$CPLEX70}''. To use a different LP 
solver or to specify a different location for CPLEX, there are a number of 
changes that need to be made.

\begin{itemize}

\item You must specify the name of and path to the library to be linked. Do 
this by right-clicking on the symphony project and choosing ``Add Files to
Project...'' Then locate the library file for the LP solver you are using
(either CPLEX or OSL). For CPLEX, you need the library called {\tt
cplex**.lib}, where ** is your CPLEX version.

\item You must set the include path for the solver header files. Do this
by right-clicking on the symphony project and choosing ``Settings...'' Then
choose the ``C/C++'' tab, and choose the category ``Preprocessor'' on the
drop-down menu.  Edit the path in the ``Additional include directories'' 
window.

\item You must set the compiler defines so that the right LP solver interface
will be used. Follow the procedure above to get to the preprocessor settings
and make sure that {\tt \_\_OSLLIB\_\_} is defined if you are using OSL or
{\tt \_\_CPLEX**\_\_} is defined if you are using CPLEX, where ** is your
version of CPLEX, i.e., {\tt \_\_CPLEX70\_\_} for CPLEX 7.0. DO NOT CHANGE
COMPILER DEFINES NOT RELATED TO THE LP SOLVER.

\end{itemize}

\item Note that there are a number of additional compiler defines that control
the functionality of SYMPHONY. These defines are described in {\tt
SYMPHONY-3.0$\backslash$Makefile}, a Unix-style make file included with the
distribution. To enable the functionality associated with a particular
compiler define, simply add it to the list of defines under the preprocessor
settings, as above.

\item You must also be sure to have any ``.dll'' files required for your LP 
solver to be in your search path. Either move the required .dll to the
directory containing symphony.exe or add the path to the ``{\tt PATH}''
Windows environment variable.

\item Once you have the proper settings for your LP solver, choose "Build
symphony.exe" from the ``Build'' menu. This should successfully build the
executable.

\item To test the executable, right click on the symphony project, go to the
``Debug'' tab and set the Program arguments to ``{\tt -F
SYMPHONY-3.0$\backslash$sample.vrp -N 5 -u 522.}'' Note that command-line
switches are Unix-style. The argument to -N is the number of routes that
should be used in the solution and the -u argument supplies the solver with an
initial upper bound.

\item Now choose ``Execute'' from the ``Build'' menu and the solver should 
solve the sample VRP file.

\item Note that there is some functionality missing from the Windows
version. Most prominently, the timing functions do not work. I suppose this
functionality should be easy to add. In addition, the Windows
version will only run in sequential mode for a veriety of reasons. However, it
should be relatively easy to get it running in parallel if you can get PVM
working under Windows.

\end{itemize}

\subsection{Orienting Yourself}

The easiest way to get oriented is to examine the organization of the source
files (note that file names will be given Unix-style). When you unpack the
\BB\ distribution, you will notice that the source files are organized along
the lines of the modules.  There is a separate directory for each
module---master ({\tt Master}), tree manager ({\tt TreeManager}), cut
generator ({\tt CutGen}), cut pool ({\tt CutPool}), and LP solver ({\tt
LP}). In addition, there is a directory called {\tt DrawGraph} and a directory
called {\tt Common} that also contain source files. The {\tt DrawGraph}
directory provides an interface from \BB\ to the {\em Interactive Graph
Drawing} software package developed by Marta Es\"o. This is an excellent
utility for graphical display and debugging. The {\tt Common} directory
contains source code for functions used by multiple modules.

Within each module's directory, there is a primary source file
containing the function {\tt main()} (named {\tt *.c} where * is the
module name), a source file containing functions related to
inter-process communication (named {\tt *\_proccomm.c}) and a file
containing general subroutines used by the module (named {\tt
*\_func.c}). The master is the exception and is organized slightly
differently. The LP process source code is further subdivided due to
the sheer number of functions.

The {\tt include} directory contains the header files. Corresponding
to each module, there are three header files, one containing internal
data structures and function prototypes associated with the module
(named {\tt *.h} where * is the module name), one containing the data
structures for storing the parameters (these are also used by the
master process), and the third containing the function prototypes for
the user functions (name {\tt *\_u.h}). By looking at the header files, you
should get a general idea of how things are laid out.

In addition to the subdirectories corresponding to each module, there are
subdirectories corresponding to applications. The sample application is
contained in the directory {\tt Vrp/}. The files containing function stubs
that can be filled in to create a new application are contained in the
directory {\tt SYMPHONY-3.0/User/}. There is one file for each module,
initially called {\tt User/*/*\_user.c}. The primary thing that
you, as the user, need to understand to build an application is how to fill in
these stubs. That is what the second section of this manual is about.

\subsection{Writing the User Functions}

The majority of the user functions are called from either the master
process or the LP process. For these two modules, user functions are
invoked from so-called {\em wrapper functions} that provide the
interface. Each wrapper function is named {\tt *\_u()} , where {\tt *}
is the name of the corresponding user function, and is defined in a
file called {\tt *\_wrapper.c}. The wrapper function first collects
the necessary data and hands it to the user by calling the user
function. Based on the return value from the user, the wrapper then
performs any necessary post-processing. Most user functions are
designed so that the user can do as little or as much as she likes.
Where it is feasible, there are default options that allow the user to
do nothing if the default behavior is acceptable. This is not possible
in all cases and the user must provide certain functions, such as
separation.

In the next section, the user functions will be described in detail.
The name of every user written function starts with {\tt user\_}.
There are three kinds of arguments:
\begin{description}
\item[\rm IN:] An argument containing information that the user might need
to perform the function.
\item[\rm OUT:] A pointer to an argument in which the user should
return a result (requested data, decision, etc.) of the function. 
\item[\rm INOUT:] An argument which contains information the user might need,
but also for which the user can change the value.
\end{description}
The return values from each function are as follows:
\begin{description}
\item[Return values:] \hfill

\begin{tabular}{lp{310pt}} 

{\tt ERROR} & Error in the user function. Printing an error message is
the user's responsibility. Depending on the work the user function was
supposed to do, the error might be ignored (and some default option
used), or the process aborts. \\

{\tt USER\_AND\_PP} & The user implemented both the user function and
post-processing (post-processing by \BB\ will be skipped).\\

{\tt USER\_NO\_PP} & The user implemented the user function only. \\

{\tt DEFAULT} & The default option is going to be used (the default is one of
the built-in options, \BB\ decides which one to use based on initial parameter
settings and the execution of the algorithm). \\

{\tt built\_in\_option1 } & \\
{\tt built\_in\_option2 } ... & The specified built-in option will be used.\\
\end{tabular}

\item[Notes:] \hfill
\begin{itemize}
\vspace{-3ex}

\item Sometimes an output is optional. This will always be noted in the
function descriptions.

\item If an array has to be returned (i.e., the argument is {\tt type
**array}) then (unless otherwise noted) the user has to allocate space
for the array itself and set {\tt *array} to be the array allocated.
If an output array is optional then the user {\em must not} set {\tt
*array} for the array she is not going to fill up because this is how
\BB\ decides which optional arrays are filled up.

\item Some built-in options are implemented so that the user can invoke them
directly from the user function. This might be useful if, for example,
the user wants to use different built-in options at different stages
of the algorithm, or if he wants to do the post-processing himself but
does not want to implement the option itself.
\end{itemize}

\end{description}

\subsection{Data Structures}

\subsubsection{Internal Data Structures}

With few exceptions, the data structures used internally by \BB\
are undocumented and most users will not need to access them
directly. However, if such access is desired, a pointer to the main data
structure used by each of the modules can be obtained simply by calling
the function {\tt get\_*\_ptr()} where * is the appropriate module (see the
header files). This function will return a pointer to the data
structure for the appropriate module. Casual users are advised against
modifying \BB's internal data structures directly.

\subsubsection{User-defined Data Structures}

The user can define her own data structure for each module to maintain
problem-specific data and any other information the user needs access
to. A pointer to this data structure is maintained by \BB\ and is
passed to the user as an argument to each user function. Since
\BB\ knows nothing about this data structure, it is up to the user
to allocate it, maintain it, and free it as required.

\subsection{Inter-process Communication for Distributed Computing}
\label{communication}
While the implementation of \BB\ strives to shield the user from
having to know anything about communications protocols or the
specifics of inter-process communication, it may be necessary for the
user to pass information from one module to another in some
cases---for instance, if the user must pass problem-specific data to
the LP process after reading them in from a data file. In cases where
this might be appropriate, user functions are supplied in pairs---a
{\em send} function and a {\em receive} function. All data are sent in
the form of arrays of either type {\tt char}, {\tt int}, or {\tt
double}, or as strings. To send an array, the user has simply to
invoke the function {\tt send\_?\_array(? *array, int length)} where ?
is one of the previously listed types. To receive that array, there is
a corresponding function called {\tt receive\_?\_array(? *array, int
length)}. When receiving an array, the user must first
allocate the appropriate amount of memory. In cases where variable
length arrays need to be passed, the user must first pass the length
of the array (as a separate array of length one) and then the array
itself. In the receive function, this allows the length to be received
first so that the proper amount of space can be allocated before
receiving the array itself. Note that data must be received in exactly
the same order as it was passed, as data is read linearly into and out
of the message buffer. The easiest way to ensure this is done properly
is to simply copy the send statements into the receive function and
change the function names. It may then be necessary to add some
allocation statements in between the receive function calls.

\subsection{The LP Engine}

\BB\ requires the use of a third-party callable library to solve the LP 
relaxations once they are formulated. Currently, ILOG's
\htmladdnormallink{CPLEX}{http://www.cplex.com}$^{\copyright}$
and IBM's
\htmladdnormallink{OSL}{http://www-4.ibm.com/software/data/bi/osl/index.html}
are the available options. Be forewarned that OSL's interface is not
tested and has exhibited suboptimal performance. Any LP solver with
the appropriate capabilities can be interfaced with \BB\ by writing a
set of interface routines contained in the file {\tt LP/lp\_solver.c}.
Once the interface routines are written, the make file must be
modified to link with the new LP solver.

\subsection{Compiling Your Application}

\subsubsection{Unix Operating Systems}

Once the user functions are filled in, all that remains is to compile the
application. The distribution comes with two make files that facilitate this
process. The primary make file resides in the {\tt SYMPHONY-3.0/} directory.
The user make file resides in the user's subdirectory, initially called {\tt
SYMPHONY-3.0/User/}. There are a number of variables that must be set in the
primary make file. First ensure that the sample application compiles by
following the instructions in Section \ref{getting_started}. Then modify the
makefile to reflect the new {\tt \$(USERROOT)} ({\tt SYMPHONY-3.0/User} by
default). When you are ready, type ``{\tt make}'' to make the executables.
\BB\ will create three new subdirectories---{\tt SYMPHONY-3.0/User/obj.*}, 
{\tt SYMPHONY-3.0/User/bin.*}, and {\tt SYMPHONY-3.0/User/dep.*} where *
is a the current architecture (defined in the make file).

\paragraph{Working with PVM.}
\label{PVM}
To compile a distributed application, it is necessary to install PVM.
The current version of PVM can be obtained at {\tt
\htmladdnormallink{http://www.csm.ornl.gov/pvm/}
{http://www.csm.ornl.gov/pvm/}}. It should compile and install without
any problem. You will have to make a few modifications to your {\tt
.cshrc} file, such as defining the {\tt PVM\_ROOT} environment
variable, but this is all explained clearly in the PVM documentation.
Note that all executables (or at least a link to them) must reside in
the {\tt \$PVM\_ROOT/bin/\$PVM\_ARCH} directory in order for parallel
processes to be spawned correctly. The environment variable {\tt
PVM\_ARCH} is set in your {\tt .cshrc} file and contains a string
representing the current architecture type. To run a parallel
application, you must first start up the daemon on each of the
machines you plan to use in the computation. How to do this is also
explained in the PVM documentation.

\paragraph{Communication with Shared Memory.}
\label{shared}
In the shared memory configuration, it is not necessary to use
message passing to move information from one module to another since
memory is globally accessible. In the few cases where the user would
ordinarily have to pass information using message passing, it is
easiest and most efficient to simply copy the information to the new
location. This copying gets done in the {\em send} function and hence
the {\em receive} function is never actually called. This means that
the user must perform all necessary initialization, etc. in the send
function. This makes it a little confusing to write source code which
will work for all configurations. However, the confusion should be
cleared up by looking at the sample application, especially the file
{\tt SYMPHONY-3.0/Vrp/Master/vrp.c}.

\paragraph{Configuring the Modules.}
\label{configuration}
In the {\tt make} file, there are
four variables that control which modules run as separate executables
and which are called directly in serial fashion. The variables are as
follows:
\begin{description}
        \item [COMPILE\_IN\_CG:] If set to {\tt TRUE}, then the cut generator
        function will be called directly from the LP in serial
        fashion, instead of running as a separate executable. This is
        desirable if cut generation is quick and running it in
        parallel is not worth the price of the communication overhead.
        \item [COMPILE\_IN\_CP:] If set to {\tt TRUE}, then the cut
        pool(s) will be maintained as a data structure auxiliary to the
        tree manager. 
        \item [COMPILE\_IN\_LP:] If set to {\tt TRUE}, then the LP
        functions will be called directly from the tree manager. When
        running the distributed version, this
        necessarily implies that there will only be one active
        subproblem at a time, and hence the code will essentially be
        running serially. IN the shared-memory version, however, the
        tree manager will be threaded in order to execute subproblems 
        in parallel.
        \item [COMPILE\_IN\_TM:] If set to {\tt TRUE}, then the tree
        will be managed directly from the master process. This is only
        recommended if a single executable is desired (i.e.~the three
        other variables are also set to true). A single executable is
        extremely useful for debugging purposes.
\end{description}
These variables can be set in virtually any combination, though some
don't really make much sense. Note that in a few user functions that
involve process communication, there will be different versions for
serial and parallel computation. This is accomplished through the use
of {\tt \#ifdef} statements in the source code. This is well documented
in the function descriptions and the in the source files containing
the function stubs. See also Section \ref{shared}.

\paragraph{Executable Names.}
\label{exe_names}
In order to keep track of the various possible configurations,
executable and their corresponding libraries are named as follows. For
the fully distributed version, the names are {\tt master}, {\tt tm},
{\tt lp}, {\tt cg}, and {\tt cp}. For other configurations, the
executable name is a combination of all the modules that were compiled
together joined by underscores. In other words, if the LP and the cut
generator modules were compiled together (i.e.~{\tt COMPILE\_IN\_CG}
set to {\tt TRUE}), then the executable name would be ``{\tt lp\_cg}''
and the corresponding library file would be called ``{\tt
liblp\_cg.a}.'' You can rename the executables as you like. However, if
you are using PVM to spawn the modules, as in the fully distributed
version, you must set the parameters {\tt *\_exe} in the parameter
file to the new executable names. See Section \ref{tm_params} for
information on setting parameters in the parameter file.

\subsubsection{Microsoft Windows}

First, follow the instructions for compiling the sample application in Section
\ref{getting_started} to ensure you have the proper settings. Once the stub 
files in the {\tt SYMPHONY-3.0$\backslash$User} hierarchy are filled in, use
the project ``{\tt vrp}'' as a template to create a new project called ``{\tt
user}'' that compiles the files in {\tt SYMPHONY-3.0$\backslash$User} and
creates a library. Modify the project ``{\tt symphony}'' to compile and link
with the new user library.

\subsection{Debugging Your Application}

Much of this section applies to Unix operating systems. However, it may
also be useful for Windows users.

\subsubsection{The First Rule}

\BB\ has many built-in options to make debugging easier. The most
important one, however, is the following rule. {\bf It is easier to
debug the fully sequential version than the fully distributed
version}. Debugging parallel code is not terrible, but it is more
difficult to understand what is going on when you have to look at the
interaction of several different modules running as separate
processes. This means multiple debugging windows which have to be
closed and restarted each time the application is re-run. For this
reason, it is highly recommended to develop code that can be compiled
serially even if you eventually intend to run in a fully distributed
environment. This does make the coding marginally more complex, but
believe me, it's worth the effort. The vast majority of your code will
be the same for either case. Make sure to set the compile flag to
``{\tt -g}'' in the make file.

\subsubsection{Debugging with PVM}
\label{debugging-PVM}
If you wish to venture into debugging your distributed application,
then you simply need to set the parameter {\tt *\_debug}, where * is
the name of the module you wish to debug, 
to the value ``4'' in the parameter file (the number ``4'' is chosen
by PVM). This will tell PVM to spawn the particular process or
processes in question under a debugger. What PVM actually does in this
case is to launch the script {\tt \$PVM\_ROOT/lib/debugger}. You will
undoubtedly want to modify this script to launch your preferred
debugger in the manner you deem fit. If you have trouble with this,
please send e-mail to the list serve (see Section \ref{resources}).

It's a little tricky to debug interacting parallel processes, but you
will quickly get the idea. The main difficulty is in that the order of
operations is difficult to control. Random interactions can occur when
processes run in parallel due to varying system loads, process
priorities, etc. Therefore, it may not always be possible to duplicate
errors. To force runs that you should be able to reproduce, make sure
the parameter {\tt no\_cut\_timeout} appears in the parameter file or
start \BB\ with the ``{\tt -a}'' option. This will keep the cut
generator from timing out, a major source of randomness. Furthermore,
run with only one active node allowed at a time (set {\tt
max\_active\_nodes} to ``1''). This will keep the tree search from
becoming random. These two steps should allow runs to be reproduced.
You still have to be careful, but this should make things easier.

\subsubsection{Using {\tt Purify} and {\tt Quantify}}

The make file is already set up for compiling applications using {\tt
purify} and {\tt quantify}. Simply set the paths to the executables
and type ``{\tt make pall}'' or ``{\tt p*}'' where * is the module you
want to purify. The executable name is the same as described in
Section \ref{exe_names}, but with a ``p'' in front of it. To tell PVM
to launch the purified version of the executables, you must set the
parameters {\tt *\_exe} in the parameter file to the purified
executable names. See Section \ref{tm_params} for information on
setting parameters in the parameter file.

\subsubsection{Checking the Validity of Cuts and Tracing the Optimal Path}
\label{debugging}
Sometimes the only evidence of a bug is the fact that the optimal
solution to a particular problem is never found. This is usually
caused by either (1) adding an invalid cut, or (2) performing an
invalid branching. There are two options available for discovering
such errors. The first is for checking the validity of added cuts.
This checking must, of course, be done by the user, but \BB\ can
facilitate such checking. To do this, the user must fill in the
function \hyperref{{\tt user\_check\_validity\_of\_cut()}} {{\tt
user\_check\_validity\_of\_cut()} (see Section
}{)}{user_check_validity_of_cut}. THIS function is called every time a
cut is passed from the cut generator to the LP and can function as an
independent verifier. To do this, the user must pass (through her own
data structures) a known feasible solution. Then for each cut passed
into the function, the user can check whether the cut is satisfied
by the feasible solution. If not, then there is a problem! Of course,
the problem could also be with the checking routine. To see how this is
done, check out the sample application file {\tt Vrp/cg\_user.c}.
After filling in this function, the user must recompile everything
(including the libraries) after uncommenting the line in the make file
that contains ``{\tt BB\_DEFINES += -DCHECK\_CUT\_VALIDITY}.'' Type
``{\tt make clean\_all}'' and then ``{\tt make}.''

Tracing the optimal path can alert the user when the subproblem which
admits a particular known feasible solution (at least
according to the branching restrictions that have been imposed so far)
is pruned. This could be due to an invalid branching. Note that this
option currently only works for branching on binary variables. To use
this facility, the user must fill in the function \hyperref{{\tt
user\_send\_feas\_sol()}} {{\tt user\_send\_feas\_sol()} (see Section
}{)}{user_send_feas_sol}. All that is required is to pass out an array
of user indices that are in the feasible solution that you want to
trace. Each time the subproblem which admits this feasible solution is
branched on, the branch that continues to admit the solution is
marked. When one of these marked subproblems is pruned, the user is
notified.

\subsubsection{Using the {\tt Interactive Graph Drawing} Software}
\label{IGD}
The Interactive Graph Drawing (IGD) software package is included with
\BB\ and \BB\ facilitates its use through interfaces with the
package. The package, which is a Tcl/Tk application, is extremely
useful for developing and debugging applications involving graph-based
problems. Given display coordinates for each node in the graph, IGD
can display support graphs corresponding to fractional solutions with or
without edge weights and node labels and weights, as well as other
information. Furthermore, the user can interactively modify the graph
by, for instance, moving the nodes apart to ``disentangle'' the
edges. The user can also interactively enter violated cuts through the
IGD interface.

To use IGD, you must have installed PVM since the drawing window runs
as a separate application and communicates with the user's routines
through message passing. To compile the graph drawing application,
type ``{\tt make dglib dg}'' in the \BB\ root directory. The user
routines in the file {\tt dg\_user.c} can be filled in, but it is not
necessary to fill anything in for basic applications. 

After compiling {\tt dg}, the user must write some subroutines that
communicate with {\tt dg} and cause the graph to be drawn.
Regrettably, this is currently a little more complicated than it needs
to be and is not well documented. However, by looking at the sample
application, it is relatively easy to see how it should be done. To
enable graph drawing, put the line {\tt do\_draw\_graph 1} into the
parameter file or use the {\tt -d} command line option.

\subsubsection{Other Debugging Techniques}

Another useful built-in function is MakeMPS, which will write the
current LP relaxation to a file in MPS format. This file can then be
read into the LP solver interactively or examined by hand for errors.
Many times, CPLEX gives much more explicit error messages
interactively than through the callable library. The form of the
function is 
\begin{verbatim}
void MakeMPS(LPData *lp_data, int bc_index, int iter_num)
\end{verbatim}
The matrix is written to the file ``{\tt
matrix.[bc\_index].[iter\_num].mps}'' where {\em bc\_index} is the
usually passed as the index of the current subproblem and {\em
iter\_num} is the current iteration number. These can, however, be any
numbers the user chooses. If \BB\ is forced to abandon solution
of an LP because the LP solver returns an error code, the current LP
relaxation is automatically written to the file ``{\tt
matrix.[bc\_index].[iter\_num].mps}'' where {\em bc\_index} is the
index of the current subproblem and {\em iter\_num} is the current
iteration number. MakeMPS can be called using breakpoint code to
examine the status of the matrix at any point during execution.

Logging is another useful feature. Logging the state of the search tree can
help isolate some problems more easily. See Section \ref{tm_params}
for the appropriate parameter settings to use logging.

\subsection{Controlling Execution and Output}
\label{output}
Calling \BB\ with no arguments simply lists all command-line options.
Most of the common parameters can be set on the command line. Usually
it is easier to use a parameter file. To invoke \BB\ with a parameter
file type ``{\tt master -f filename ...}'' where filename is the name
of the parameter file. The format of the file is explained in Section
\ref{parameter_file}. 

The output level can be controlled through the use of the verbosity
parameter. Setting this parameter at different levels will cause
different progress messages to be printed out. Level 0 only prints out
the introductory and solution summary messages, along with status
messages every 10 minutes. Level 1 prints out a message every time a
new node is created. Level 3 prints out messages describing each
iteration of the solution process. Levels beyond 3 print out even more
detailed information.

There are also two possible graphical interfaces. For graph-based
problems, the Interactive Graph Drawing Software allows visual display
of fractional solutions, as well as feasible and optimal solutions
discovered during the solution process. For all types of problems,
VBCTOOL creates a visual picture of the branch and cut tree, either
in real time as the solution process evolves or as an emulation from a
file created by
\BB. See Section \ref{tm_params} for information on how to use VBCTOOL
with SYMPHONY. Binaries for VBCTOOL can be obtained at \\ 
{\tt \htmladdnormallink
{http://www.informatik.uni-koeln.de/ls\_juenger/projects/vbctool.html}
{http://www.informatik.uni-koeln.de/ls\_juenger/projects/vbctool.html}}.


\subsection{Other Resources}
\label{resources}
There is a \BB\ user's list serve for posting questions/comments.
To subscribe, send ``{\tt subscribe symphony-users}'' to
{\tt \htmladdnormallink{majordomo@branchandcut.org}
{mailto:majordomo@branchandcut.org}}. There is also a Web site for
\htmladdnormallink{SYMPHONY}{http://branchandcut.org/SYMPHONY} 
\begin{latexonly}
at {\tt http://branchandcut.org/SYMPHONY}
\end{latexonly}.  
Bug reports can be sent to \\
{\tt \htmladdnormallink{symphony-bugs@branchandcut.org}
{mailto:symphony-bugs@branchandcut.org}}.


